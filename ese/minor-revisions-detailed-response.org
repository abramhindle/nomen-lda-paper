#+Title: Detailed Response to reviews for #EMSE616R1
#+AUTHOR: Abram Hindle, Neil A. Ernst, Michael W. Godfrey, John Mylopoulos

* Detailed Response to reviews for #EMSE616R1

We would like to thank you for your thorough reviews. In our revised
paper, we have addressed the minor revision points of the 4 reviewers.
We now explain how we have amended the paper to incorporate the
revisions requested.

The format is Review text followed by "Response:" which includes our
comments on how we dealt with this review revision. If it was a small
edit we answered with "addressed".

* Minor Revisions -- Detailed Responses
** Reviewer 2 [23/23]
   - [X] text label size on Fig 2, 6
     The text labels in Figures 2, 6, and to some extent 9, are
     virtually unreadable on the printed copy. I understand that many
     people will view this paper in its electronic form, but shouldn't
     the text be readable on a print out as well? If the authors are
     having trouble increasing the size of the text in their diagram
     program (without increasing the size of the figure to 3 pages), I
     suggest they use pdf or image editing software to white out the
     current labels & overlay larger ones. I think readability is more
     important than aesthetics at this point, and there appears to be
     plenty of room in the paper for the minor addition of larger
     labels (regardless of how the size is increased).
     - Response: We fixed the text label sizes as per the reviewers comments.
   - [X] Text label size on fig 9
     - Response: We fixed the text label sizes as per the reviewers comments.
   - [X] More on F-measure
           3.1.2: the authors do an excellent job explaining the ROC
     curves and how to interpret them. However, it is not clear what
     the reader should be getting from the F Measure results. The
     authors addressed this issue by stating the purpose of the F
     Measure results is for readers who are more familiar with F than
     ROC, to help them interpret results. This is fine, but does not
     completely fix the issue. It is possible for a reader to be
     familiar with F in general, but  not know how to interpret the
     results in this domain. If you are including a measure, please
     include at least a brief sentence of what high/low (or good/bad)
     results mean.
     - Response: in subsection {Measuring Classification Performance}
       we provided some context for F-Measure and explained some of
       the issues that face the interpretation of F-Measure.
   - [X] Figure 2: why weren't the exp1 ROC values reported? 
          Because they were so poor? This should be explained in the
     text. Even if the ROC results for exp1 are poor, the raw numbers
     at least should be reported. In a later response the authors
     state that exp1 ROC numbers COULDN'T be included in Figure 2
     because they use different NFRs. That is not the implication from
     section 3.2.3 in the text, which just states ROC PLOTS for exp1
     are omitted because results are poor. The fact that the ROC
     results for exp1 cannot be found anywhere else in the paper (or
     at least, I couldn't find them) makes it seem as if the authors
     are sweeping bad results under the rug (although I'm sure this is
     not the authors' intent).
     - Response: In section ``Analysis of the Semi-Unsupervised
       Labelling'' we explain that exp1 couldn't be directly
       calculated and compared with exp2 and exp3 because it used
       different classes, but we explained the exp1 was estimated by
       an overlap in order to compare to exp2 and exp3. Because we
       couldn't use the same technique to calculate ROC in this case 
   - [X] More on intro extensions. 
          End of intro describing major extensions: this information
     is a welcome addition, but incomplete. How is another case study
     on PgSQL different from what the previous study did? To analyze
     this software project at all, or to analyze it differently? This
     needs to be more clear for readers who haven't read the previous
     work.
     - Response: At the end of the intro we illustrated this more.
   - [X] p21, l30: "Neil's annotations" can the authors justify why
     this author's annotations are used, when they claim earlier that
     author 1 was more successful in annotating, given Figure 5? Also,
     can the authors be more consistent in how they refer to the
     annotators? For example, the second annotator is referred to as
     author #2, Neil, and Ernst. Each section seems to use its own
     naming convention, which is very confusing for the reader.
     - Response: throughout the text we clarified why, which author,
       and we numbered the authors instead of using names or titles.
   - [X] 5.5, last sentence: "other domains"
         can the authors qualify this as software projects in other
     domains, rather than implying the authors intend to apply the
     analysis to other textual artifacts outside SE. The authors claim
     to have addressed this issue, but the sentence in the revised
     submission is identical to the original submission?!? Are there
     any other reviewer comments that have this same oversight???
     - Response: We apologize for the oversight (the original sentence
       did mention consumer-facing software but that was a suffix), we
       have clarified that we mean domains of software projects and
       have provided some examples.
   - [X]  Intro, 2nd para: please define the terms NFR & project
     dashboard before using.
     - response: Addressed
   - [X] p 3, l 7: "topic trends --" the second -- needs a transition
     - response: Addressed
     word after it like and/where, or could be changed to a period.
     - response: Addressed
   - [X] p3 l9: "is further emphasized in this paper..." is this point
     really pertinent here? Aren't the authors still trying to
     motivate the purpose of the work they've undertaken in the
     current paper at this point in the intro?
     - response: Addressed
   - [X] p3, l49: bad page turn, use nonbreaking space for
     Section~4. Also bad line break for p5, l9 milestone~3.
     - response: Addressed
   - [X] p6, l35: bucket -> buckets
     - response: Addressed
   - [X]  top of p8: low-ercase is bad line break
     - response: Addressed
   - [X] p14 l15 just before 3.3: do "quality word lists" mean exp2 &
     3? Can this be made more explicit?
     - response: Addressed
   - [X]   - 3.4, p16, l47: "labels.During" -- needs space
     - response: Addressed
   - [X]    - p20, l15: "August, 2002," -> August 2002,
     - response: Addressed
   - [X]    - p21, l44: "This will group" -> This groups?
     - response: Addressed
   - [X] - p21, l47: "the path We" -> the path. We
     - response: Addressed
   - [X]    - Fig 7: Although Figure 7 is now much easier to read, I
     can't see the difference between dim gray & black on a color
     print out. However, the purple in the prior submission provided
     at least some contrast.     
     - response: Addressed
   - [X]    - p22, l46: "developers are have different" -> remove are
     - response: Addressed
   - [X]    - p23, l49: "Whereas, ..." This sentence seems to be
     - response: Addressed
     missing a clause, and does not make sense as a stand alone
     sentence.
     - response: Addressed
   - [X]    - Fig 8: can't read tgl in the upper right -- can the
     authors just plot momjian & then add the tgl label in post
     processing? The other overlapping author names aren't an issue,
     but tgl & momjian are used as examples for the reader to refer to
     in the text.
     - response: Addressed
   - [X]    - p24, l46: "be more likely take on" -> to take on
     - response: Addressed
** Reviewer 3  [6/6]
   - [X] Redo section 3
        All of my major issues have been addressed and I believe this
     paper is strong enough to accept.  However, I am concerned that
     Section 3 contains too much content and should be split into two
     sections, and thus I would support one more round of revisions.
     I only mention this because I believe a restructuring of that
     section would make this paper much more readable.  I'm still not
     convinced that the paper organization is sufficiently clear.
     Section 3 "Study Design and Execution" spans pages 6 - 17, about
     1/3 of the paper, which sticks out to me.  I would rather see
     this split into two sections (e.g., "Approach" and "Study").  In
     relation to Figure \#1 this would mean presenting the top half of
     this process in the first section and the bottom half in the
     section.  As it stands now Section 3 mixes in both approach
     (e.g., "Generating Word-Lists") with evaluation (e.g.,
     "Generating the Data").  I personally find Section 3 difficult to
     read because of this mix of approach and evaluation, and think
     that the size of the section is akin to the "Long Method" code
     smell.
     - Response: We interpreted and executed the reviewer comments as
       best we can. We split up section 3 and broke the methodology
       from the results. The result is far more readable.
   - [X] Sentence starting with "Researchers have.." is a run-on
     sentence.
     - Response: addressed
   - [X] Flow between "However, these word-lists.." and "Current topic
     modeling.." sentences is poor.
     - Response: addressed
   - [X] Why is the discussion of ROC vs F-Measure hidden in the
     section called "Creating a Validation Corpus"?  
     I would rather see this discussion in its own sub-sub-section,
     "3.1.3 Measuring Classification Performance". This makes this
     section easier to find for future readers.
     - Response: addressed
   - [X] You may devote too much space to the ROC over F-Measure
     argument given that you provide both measures throughout the rest
     of the paper.
     - Response: We feel that the true negatives are important and ignored by
       F-measure, especially in the face of class imbalance. While we
       attempted to tone this down a bit we still had to motivate ROCs
       use over F-measure for our analysis, regardless of the fact
       that we show both.
   - [X] ROC claim
         I think that average ROC values of between 0.6 and 0.8 cannot
     be considered "performing well", as described in the Conclusion.
     This claim should be softened, even as slightly as to change it
     to "performed noticeably better than random".
     - Response: We changed the language the match this reviewer's comments.
** Reviewer 4 [4/4] 
   - [X]  Discussion of multiple labels
       I did not find the discussion arguing that multiple labels in
       Pg were caused by too many terms in each 30-day window to be
       convincing. It seems that the overall increase in change-log
       words is less than double of that in MySQL (165K words in Pg vs
       101K in MySQL), yet the differences in topic overlap appear to
       be quite notable. The argument also appears to conflict with an
       earlier statement that N=20 was chosen because the overlap in
       topics (two topics sharing most frequent terms) was infrequent.
     - Response: In section Study Design we added an explanation that N=20 was
       kept for consistency reasons, because the original study used
       MaxDB and MySQL, PostgreSQL was added later and we wanted to
       maintain methodological consistency.  Further more in section
       ``Analysis of the Supervised Labelling'' we added information
       that shows that PostgreSQL commit log messages contain more
       information than MaxDB or MySQL.
       We also added an observation about the pattern of PostgreSQL
       commit log messages.

   - [X] There appear to be some discrepancies in numbers. 
         According to Table 2, there were 640 topics in Pg. 20 topics
         per 30-day windo 32 30-day windows for Pg, yet the time
         period appears to contain fewer such windows.  For the other
         two projects the numbers of topics are not even divisible
         by 20. Some clarification would help. It appears that Pg has
         625 of 640 topics with at least two labels: testability and
         correctness based on exp1. It may be worth giving he reader
         some idea about how many topics have exactly one label (the
         table shows numbers for none and at least one label)
       - Response: Topics get dropped if documents aren't actually
         relevant to the topic enough to warrant keeping the topic. A
         topic will stay if documents meet a threshold of relevance,
         if documents do not make that relevance the topic is removed.
         In ``Generating the Data and in Analysis of the
         Semi-Unsupervised Labelling'' we explain this filtering.

   - [X] Validity on authors
        Although briefly noted in the validity section, the two most
        frequent committers may be defining the global distribution of
        NFR topics in Pg, if, for example they have produced the bulk
        of commits for the topic extraction. This, perhaps, could be
        mentioned earlier with a fraction of commits these two authors
        produced.
     - Response: In section {RQ3: Do Different Developers Work on Different
       NFRs?} we added text that showed the proportions of the top 3
       authors and later mentioned this domination of the the global
       NFR topic distribution by the top 3 authors.
       


   - [X] Word more cautiously
        I still think that the overall work is primarily to introduces
        the topic analysis and how it might be used in the context of
        software maintenance activities as the low inter-rater
        agreement makes one ask the question: are these topics real?
        In particular, the performance of the supervised learning
        algorithms and, more generally, the interpretation of the NFR
        topic trends could be worded a bit more cautiously to be
        consistent with that lack of certainty if the labeling is
        reflective of the underlying intent.
     - Response:
       - We toned down the language in the conclusion regarding the
         ROC scores
       - In section ``Understanding Software Maintenance Activities''
         we mention that these topic trend plots are showing an
         interpretation.
       - We walked through the paper and tried to address any issues
         relevant to review and tried to add caution to the language.

* Summary

We believe we have addressed the reviews and the new structure of the
paper aides readability.  We feel that this new revision is far more
robust and should ease the concerns of the reviewers.

Thank you,

Abram, Neil, Michael and John




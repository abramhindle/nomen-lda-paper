% \section{Background}

% We provide a brief overview of software repository mining and information retrieval.
% This work is related to the mining software repositories (MSR)~\cite{msr} research community as it deals expressly with analyzing a project's source control repository, and the messages associated with revisions therein.

% \subsection{Definitions}
% We will use the following terminology in the rest of the paper.
% A \emph{message} is a block of text written by a developer. In this
% paper, messages will be the CVS and BitKeeper commit log comments made
% when a developer commits changes to files in a repository. A \emph{word
%   distribution} is the summary of a message by word count. Each word
% distribution will be over the words in all messages. However, most
% words will not appear in each message. A word distribution is effectively
% a word count divided by the message size. A \emph{topic} is a word
% distribution, i.e., a set of words that form a word distribution that is
% unique and independent within the set of documents in our total
% corpus. One could think of a topic as a distribution of the centroid
% of a group of messages. In this paper we often summarize topics by the
% top ten most frequent words of their word distribution.  A \emph{trend}
% is one or more similar topics that recur over time.  Trends are
% particularly important, as they indicate long-lived and recurring
% topics that may provide key insights into the development of the
% project. A \emph{label} is part of a title we attach to a topic, whether manually or automatically.

% % area under the ROC 
% % check
% 

% \shrink
% \subsection{Topic and Concept Extraction}
% Topic extraction, sometimes called concept extraction, uses tools such as Latent Dirichlet Allocation (LDA)~\cite{Blei2003} and Latent Semantic Indexing (LSI) to extract



% independent word distributions (topics) from
% 	documents (commit log comments). Many researchers~\cite{marcus04wcre,Poshyvanyk2007,lukins08wcre,Linstead2007} have applied tools like LSI and LDA to mining software repositories, in particular analyzing source code, bugs or developer comments.
% \begin{comment}
% Ideally these extracted topics 
% correlate with actual development topics that the developers discussed
% during the development of the software project. For example, our technique might identify a collection of words: ``list next has iterator add'', which we then identify (manually) as concerning the topics of collections and the Iterator pattern.
% \end{comment}

% Typically a topic analysis tool like LDA will try to find $N$ independent word distributions found within the word distributions of all the messages. Linear combinations of these $N$ word distributions are then meant to be able to recreate the word distributions of all of the underlying messages. These $N$ word distributions effectively form topics: cross cutting collections of words relevant to one or more documents. Our problem is that these topics are not easy to interpret, as the underlying pattern is not clear. We feel that automatic labelling or naming of these topics would be helpful with respect to interpreting the subject of a topic. LDA extracts topics in an unsupervised manner; the algorithm relies solely on the source data with no human intervention.

% In topic analysis a single document, such as a commit message, can be related to multiple topics. Representing documents as a mixture of topics maps well to source code repository commits, which often have more than one purpose~\cite{Hindle09ICSM}.  A topic represents both a word distribution and a group of commit log comments that are related to each other by their content.  In this paper a topic is a set of tokens extracted from commit messages found within a project's source control system (SCS).

% One issue that arises with use of unsupervised techniques is how to label the topics. While the topic models themselves are generated automatically, what to make of them is less clear. For example, in our previous work~\cite{Hindle09ICSM}, as well as in Baldi et al.~\cite{Baldi2008}, topics are named manually: human experts read the highest-frequency members of a topic and assign a keyword accordingly. E.g., for the word list \emph{``listener change remove add fire''}, Baldi et al. assign the keyword \emph{event-handling}. The labels are reasonable enough, but still require an expert in the field to determine them. Our technique is automated, because we match keywords from WordNet~\cite{Fellbaum1998} to words in the topic model. 

% \shrink
% \subsection{Supervised learning}
% While unsupervised techniques (LSI and LDA are both unsupervised) are appealing in their lack of human intervention, and thus lower effort, supervised learners have the advantage of domain knowledge which typically means improved results. In supervised learning, the data is divided into slices. One slice is manually annotated by the domain expert, and the classifications he/she determines are applied to the remaining slices. In this paper, we employ the WEKA~\cite{weka09} and Mulan~\cite{mulan} machine learning frameworks in order to test supervised learning.



% To gather the data, we first extract the commit log comments from a  project's SCS repository. We filter out stop words and produce word distributions from these messages. These distributions are bucketed
% into 30-day non-overlapping windows (periods). Each window is subject to LDA analysis. We arbitrarily create 20 topics, vectors of important words according to LDA. 


% Using the F-Measure, the weighted average of precision and recall, where 1 is perfect, our best results are 0.6, a few at 0.4, and most around 0.2. We achieved similar results using the Matthew's correlation coefficient (used to measure efficacy where classes are of different sizes) and ROC.  

% \begin{table*}%[ht]
% 	\centering

% \begin{tabular}{c|c|c|c|c|c|c}
% \toprule
% Experiment & Label & F1 & MCC & Precision & Recall & ROC  \\ 
% \midrule
% MaxDB exp2 & portability & 0.228 & 0.182 & 0.520 & 0.146 & 0.553 \\ 
%  & efficiency & 0.217 & 0.125 & 0.237 & 0.200 & 0.558\\ 
%  & reliability & 0.380 & 0.340 & 0.246 & 0.829 & 0.765 \\ 
%  & functionality & 0.095 & 0.083 & 0.250 & 0.059 & 0.521 \\ 
%  & maintainability & 0.092 & 0.123 & 0.571 & 0.050 & 0.520 \\ 
%  & usability & 0.175 & 0.138 & 0.113 & 0.389 & 0.620 \\ 
%  & total & 0.236 & 0.127 & 0.248 & 0.225 & 0.561 \\ 
% \midrule
% MySQL exp2 & portability & 0.138 & 0.211 & 1.000 & 0.074 & 0.537 \\ 
%  & efficiency & 0.345 & 0.327 & 0.476 & 0.270 & 0.625 \\ 
%  & reliability & 0.425 & 0.287 & 0.348 & 0.545 & 0.669 \\ 
%  & functionality & 0.025 & 0.006 & 0.571 & 0.013 & 0.501 \\ 
%  & maintainability & 0.000 & 0.000 & 0.000 & 0.000 & 0.500 \\ 
%  & usability & 0.175 & 0.135 & 0.200 & 0.156 & 0.560 \\ 
%  & total & 0.167 & 0.095 & 0.403 & 0.105 & 0.527 \\ 
% \midrule
% MaxDB exp3 & portability & 0.472 & 0.286 & 0.402 & 0.573 & 0.660 \\ 
%  & efficiency & 0.223 & 0.068 & 0.130 & 0.778 & 0.549 \\ 
%  & reliability & 0.257 & 0.196 & 0.149 & 0.927 & 0.652 \\ 
%  & functionality & 0.236 & 0.187 & 0.138 & 0.824 & 0.665 \\ 
%  & maintainability & 0.338 & 0.112 & 0.266 & 0.463 & 0.566 \\ 
%  & usability & 0.108 & 0.094 & 0.057 & 0.944 & 0.595 \\ 
%  & total & 0.258 & 0.093 & 0.160 & 0.671 & 0.568 \\ 
% \midrule
% MySQL exp3 & portability & 0.413 & 0.170 & 0.564 & 0.325 & 0.574 \\ 
%  & efficiency & 0.158 & 0.105 & 0.089 & 0.703 & 0.608 \\ 
%  & reliability & 0.388 & 0.240 & 0.260 & 0.758 & 0.660 \\ 
%  & functionality & 0.652 & 0.240 & 0.655 & 0.649 & 0.620 \\ 
%  & maintainability & 0.203 & 0.007 & 0.240 & 0.175 & 0.503 \\ 
%  & usability & 0.105 & 0.013 & 0.057 & 0.688 & 0.513 \\ 
%  & total & 0.362 & 0.076 & 0.284 & 0.499 & 0.544 \\
% \bottomrule
% \end{tabular}
% 	\caption{Results for automatic topic labelling. F1 = f-measure, MCC = Matthew's correlation coeff., ROC = Area under ROC curve}
% 	\label{tbl:maxdb-unsup-results}
% \end{table*}

% Cribbed from: 
% src/validate/latex-out/best-learner-per-project-roc-table.txt
% \begin{table*}
% \centering
% \begin{tabular}{ccc|rrrr}
% \toprule
% Label & Project & Learner & ROC & ZeroR Acc. & ZeroR. Diff & F1\\
% \midrule

% portability &  MySQL &  NaiveBayesMultinomial &  0.74 &  58.53 &  11.43 &  0.62 \\
% efficiency &  MySQL &  NaiveBayes &  0.67 &  93.69 &  -7.68 &  0.23 \\ 
% reliability &  MySQL &  NaiveBayes &  0.73 &  83.11 &  -12.80 &  0.41 \\ 
% functionality &  MySQL &  DMNBtext &  0.81 &  54.44 &  21.67 &  0.77  \\ 
% maintainability &  MySQL &  DMNBtext &  0.78 &  76.62 &  3.41 &  0.32  \\ 
% usability &  MySQL &  NaiveBayes &  0.75 &  94.54 &  -5.80 &  0.21  \\ 
%  \hline 
% portability &  MaxDB &  NaiveBayes &  0.84 &  77.12 &  2.06 &  0.61 \\ 
% efficiency &  MaxDB &  NaiveBayes &  0.62 &  88.43 &  -11.31 &  0.25  \\ 
% reliability &  MaxDB &  DMNBtext &  0.84 &  89.46 &  3.86 &  0.57  \\ 
% functionality &  MaxDB &  NaiveBayes &  0.67 &  91.26 &  -6.94 &  0.31  \\ 
% maintainability &  MaxDB &  NaiveBayes &  0.70 &  79.43 &  -9.25 &  0.42  \\ 
% usability &  MaxDB &  NaiveBayes &  0.56 &  95.37 &  -4.37 &  0.00  \\ 
%  \hline 


% \end{tabular}
% \caption{Per label, per project, the best learner for that label. ROC value rates learner performance, compared with the ZeroR learner ( a learner which just chooses the largest category all of the time). F1 is the F-measure for that particular learner.}
% \label{tab:best-learn-per-tag}
% \end{table*}



%Figure \ref{fig:commits}
% depicts the general process for processing and analyzing the commit messages. % \begin{table*}%[h]
%   \centering
%   \begin{tabular}{l|ccc|ccc}
%     \toprule
%     & MySQL & MySQL & MySQL &                         MaxDB & MaxDB & MaxDB  \\ 
%     Performance Metric  & BR & CLR & HOMER &                              BR & CLR & HOMER \\     
%     \midrule
%     example Subset Accuracy & 0.19 & 0.24 & 0.31 &     0.40  &  0.43  &  0.45 \\ 
%     \hline                                                                      
%     label macro-ROC & 0.74 & 0.66 & 0.64 &            NaN  &  NaN  &  NaN    \\ 
%     label macro-F1 & 0.46 & 0.30 & 0.43 &             0.32  &  0.19  &  0.23 \\ 
%     label macro-Precision & 0.41 & 0.41 & 0.49 &      0.29  &  0.31  &  0.32 \\ 
%     label macro-Recall & 0.56 & 0.26 & 0.40 &         0.38  &  0.15  &  0.20 \\ 
%     label micro-ROC & 0.81 & 0.81 & 0.77 &            0.76  &  0.66  &  0.62 \\ 
%     label micro-F1 & 0.59 & 0.53 & 0.61 &             0.39  &  0.27  &  0.34 \\ 
%     label micro-Precision & 0.51 & 0.69 & 0.66 &      0.34  &  0.42  &  0.49 \\ 
%     label micro-Recall & 0.68 & 0.43 & 0.56 &         0.47  &  0.21  &  0.26 \\ 
%     \hline                                                                      
%     rank Avg. Precision & 0.76 & 0.77 & 0.69 &          0.54  &  0.57  &  0.54 \\ 
%     rank Coverage & 1.51 & 1.47 & 1.94 &              1.40  &  1.23  &  1.43 \\ 
%     rank One-error & 0.40 & 0.39 & 0.47 &             0.81  &  0.79  &  0.81 \\ 
%     rank Ranking Loss & 0.19 & 0.18 & 0.27 &          0.41  &  0.35  &  0.41 \\ 
%     \hline
%   \end{tabular}
%   \caption{MySQL and MaxDB Mulan results per Multi-Label learner}
% \label{tab:mulan}
% \end{table*}

%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode
\documentclass[10pt, conference, compsocconf]{IEEEtran}
\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}
\usepackage{booktabs}
\defaultfontfeatures{Mapping=tex-text}
\setmainfont{Times New Roman}
%\usepackage{amsthm}
\usepackage{color}


\usepackage{comment}
\newtheorem{mydef}{definition}
\usepackage{graphicx}
\newcommand{\slbl}[1]{\textbf{\textsf{#1}}}
\usepackage{multirow}%,multicolumn}
\usepackage{slashbox}
\usepackage{wrapfig}
\usepackage{booktabs, colortbl}
\usepackage{amsmath}
%\usepackage{amsmath,amssymb}
%\usepackage{algpseudocode}
\usepackage[pdfstartview=FitH,xetex,bookmarksopenlevel=3,bookmarks=true]{hyperref} %pdfpagescrop={92 112 523 778},
%\usepackage{algorithm}%,algorithmic-fix}
\usepackage{url}
\begin{document}
 
\author{
\IEEEauthorblockN{Abram Hindle}
\IEEEauthorblockA{Department of Computer Science\\University of Waterloo\\ ahindle@uwaterloo.ca\\}
\and
\IEEEauthorblockN{Neil A. Ernst}
\IEEEauthorblockA{Department of Computer Science\\ University of Toronto\\ nernst@cs.toronto.edu}
}

\title{What's in a name? On the (automated) topic naming of software maintenance activities}

\maketitle
\thispagestyle{empty}

\begin{abstract}
  In the automated mining of software repositories, most approaches, such as topic modeling and concept location, focus on applying standard machine learning algorithms to corpora. This approach does not make any use of domain-specific information to better understand results. While too much specificity can produce non-generalizable results, too little produces broad learners that don't provide much useful detail.%There has not been any   investigation into the latent topics these classifiers represent. 
In this paper, we investigate what topics the latent topics classifier produce, and examine  whether such topics can be
  generalized across software products. We focus on nonfunctional requirements related to software quality
  as a potential generalization, since there is some shared belief
  that these qualities apply broadly across software products. We
  wanted to name topics based on a static ontology of non-functional
  requirements and software qualities. %Our overarching goal is to use   individual topic models to create a list of terms that can be shared   across software projects. 
In essence, this would be adding some form
  of context to general-purpose ontologies such as Wordnet. In this
  paper, we discuss how to use domain knowledge about software
  development in order to better annotate topics of development that
  are extracted from the change log messages in configuration
  management systems such as CVS. Since our taxonomy is global, our results allow us to compare the relative importance of particular qualities between projects.
\end{abstract}



\section{Introduction}
\begin{quote}
	\emph{What's in a name? that which we call a rose\\
	By any other name would smell as sweet;}
	\begin{flushright} -- Romeo and Juliet, II:ii\end{flushright}
\end{quote}

Unlike the poetic license Shakespeare uses, we show that names are indeed important. The rose has its own tangible properties which allow us to sense it. Unfortunately when we are dealing with data abstracted from program source code, we are already dealing with abstract entities that are not tangible.

During the development of a software project developers touch on many issues integral to the purpose and specification of the software project. These issues are often referred to as topics. These topics can sometimes be automatically extracted from the changelog comments that developers create while committing revisions to the project's source control repository. Our previous work dealt with topic-trends, topics that repeat overtime. It seemed that these trends were often nonfunctional requirements. 

Topics in this paper and the previous paper are word bags or word distributions. These word distributions are found via Latent Dirichlet Allocation~\cite{Blei2003}, which find independent word distributions shared among documents (changelog comments). The unfortunate aspect of topics that are word distributions is that they lack the tangibility of a rose. They do not self idea themselves due to their tangible properties. Topics have to be identified by interpretting the prevelant words in the word distribution and by inspecting related documents. This is tiresome when one has to handle more than one hundred different topics. It would be nice if we could, at a glance at , have some idea what the topic is about. This is the power of labelling and naming.

If a topic has a name or a label we immediately understand and register what it is. In our case the name is the bit of information, the summary, that was missing from our topics to make them useful and complete. Although we are convinced that accurate naming is not completely feasible, so we have focussed on labelling topics by non-functional requirements.

%In this paper we \textit{name} and \textit{label} topics generated by %a topic modeling technique, Latent Dirichlet %Allocation~\cite{Blei2003}. We use these topic models to train %supervised learners to classify larger datasets. We also show that %these topics can be generalized across products.

% Clean this up
Traditionally topic extraction has required manual annotation to derive domain-relevant information. This paper attempts to implement \emph{labelled topic extraction}, topic extracted are given labels relating to a cross-project taxonomy. This is an ontological approach where we attempt to relate networks of words to labels and then search for these terms in our topic. We also compare this approach to machine leaners.
%We start with an unsupervised topic learning algorithm, then manually %annotate the topics using the ontology. From the annotated topics, we %apply automated classifiers to find labels for the remaining topics. 

% XXX Not sure about this
\begin{comment}
Typically what is distinct is the software's particular functionality -- whether it offers email composition, prepares tax returns, and so on. What is common, however, is the non-functional, or quality requirements, that the software focuses on. Our approach makes use of software quality taxonomies in order to compare different projects on similar concepts. For example, how is the Security quality treated over time by product A versus product B? Does product A care about it more as it matures? Does the quality occur more often in software commits?
\end{comment}

Our contributions include:
\begin{itemize}
\item  We introduce labelled topic extraction, and demonstrate its usefulness
  compared to other approaches.
% unsure about this
\item We show that these topics can be learned and used to classify other datasets.
\item We present a number of visualizations of named topics and their trends over time to aid
  communication and analysis.
\item We use an exploratory case study of
  several open source database systems to show how named topics can be compared between projects.
\end{itemize}

We first introduce some important terminology for our work. We then describe our methodology, including our datasets, then highlight our results. We conclude with a look at related work and possible improvements.

\section{Background}

We provide a brief overview of software repository mining and information retrieval.
This work is related to the mining software repositories (MSR)~\cite{msr} research community as it deals expressly with analyzing a project's source control repository, and the messages associated with revisions therein.


\subsection{Definitions}
We will use the following terminology in the rest of the paper.
A \emph{message} is a block of text written by a developer. In this
paper, messages will be the CVS and BitKeeper commit log comments made
when the user commits changes to files in a repository. A \emph{word
  distribution} is the summary of a message by word count. Each word
distribution will be over the words in all messages. However, most
words will not appear in each message. A word distribution is effectively
a word count divided by the message size. A \emph{topic} is a word
distribution, i.e., a set of words that form a word distribution that is
unique and independent within the set of documents in our total
corpus. One could think of a topic as a distribution of the centroid
of a group of messages. In this paper we often summarize topics by the
top 10 most frequent words of their word distribution.  A \emph{trend}
is one or more similar topics that recur over time.  Trends are
particularly important, as they indicate long-lived and recurring
topics that may provide key insights into the development of the
project. A \emph{label} is part of a title we attach to a topic, whether manually or automatically.

% area under the ROC 
% check
\emph{Area of ROC Curve} is the area under the Reciever Operating Characteric curve, we will refer to this as \emph{ROC} although it is sometimes referred to as \emph{AUC}. ROC values reflect a score, similar to school letter-grades (A is 0.9, C is 0.6), reflecting how well a particular learner performed for the given data. A ROC result of 0.5 would be equivalent to a random learner (randomly classifying data). ROC maps to the more familiar concepts of precision/sensitivity and recall/specificity: it plots the true positive rate (sensitivity) vs. the false positive rate (1 - specificity). A perfect learner has a ROC value of 1.0, reflecting perfect recall and precision.

\subsection{Topic and Concept Extraction}
Topic extraction, sometimes called concept extraction, uses tools such as Latent Dirichlet Allocation (LDA)~\cite{Blei2003} and Latent Semantic Indexing (LSI) to extract
independent word distributions (topics) from
	documents (commit log comments). Many researchers~\cite{marcus04wcre,Poshyvanyk2007,lukins08wcre,Linstead2007} have applied tools like LSI and LDA for mining repositories, in particular analyzing source code, bugs or developer comments.
\begin{comments}
Ideally these extracted topics 
correlate with actual development topics that the developers discussed
during the development of the software project. For example, our technique might identify a collection of words: ``list next has iterator add'', which we then identify (manually) as concerning the topics of collections and the Iterator pattern.
\end{comments}

Typically a topic analysis tool like LDA will try to find $N$ independent word distributions found within the word distributions of all the messages. Linear combinations of these $N$ word distributions are then meant to be able to recreate the word distributions of all of the underlying messages. These $N$ word distributions effectively form topics: cross cutting collections of words relevant to 1 or more documents. Our problem is that these topics are not easy to interpret and we feel that automatic labelling or naming of these topics would be helpful with respect to interpreting what a topic is about. LDA's topics extracted in an unsupervised manner, LDA relies solely on the source data with no human intervention.

In topic analysis a single document, such as a commit message, can be related to multiple topics. Representing documents as a mixture of topics maps well to commits to source code repositories, which often have more than one purpose~\cite{hindle09icsm}.  A topic represents both a word distribution and a group of commit log comments that are related to each other by their content.  In this paper a topic is a set of tokens extracted from commit messages found within a project's source control system (SCS).

\begin{comment}
In this paper we use LDA to identify topics.  LDA works by creating topic models for each period. A topic model is a statistical summary of the important subjects for that dataset.  The input to the algorithm is a set of documents, with contents treated as a bag of words, and a parameter to set the number of topics to generate. The algorithm returns a word-document matrix relating the occurrence frequency of a 
 The idea is similar to other IR approaches like TF/IDF (term frequency/inverse document frequency), which find uncommon words that aren't found in other wordlists. These approaches are unsupervised in that they rely solely on the source data with no human intervention.
\end{comment}

One issue that arises with use of unsupervised techniques is how to label the topics. While the topic models themselves are generated automatically, what to make of them is less clear. For example, in our previous work~\cite{Hindle09ICSM}, and in \cite{Baldi2008}, topics are named manually: human experts read the highest-frequency members of a topic and assign a keyword accordingly. E.g., for the word list ``listener change remove add fire", Baldi et al. assign the keyword `event-handling'. The labels are reasonable enough, but still require some expert in the field to determine them. Our technique is automated, because we match keywords from Wordnet to words in the topic model. 

\subsection{Supervised learning}
While unsupervised techniques (LSI and LDA are both unsupervised) are appealing in their lack of human intervention, supervised learners have the advantage of domain knowledge. This typically results in improved results. In supervised learning, the data is divided into slices. One slice is manually annotated by the domain expert
In this paper, we employ the WEKA and Mulan machine learning frameworks in order to test supervised learning.
 
\section{Methodology}
 % To gather the data, we first extract the commit log comments from a
% project's SCS repository. We filter out stop words and produce word distributions from these messages. These distributions are bucketed
% into 30-day non-overlapping windows. Each window is subject to LDA analysis. %Figure \ref{fig:commits}
% depicts the general process for processing and analyzing the commit messages.

% XXX how many do we have
We wanted to compare two mature software projects, that had open repositories, within the same domain. This allows us to partially control for differences in functional requirements. We selected MySQL and MaxDB, open-source, near-commercial quality database software. MaxDB started in the late 1970s as a research project, and was later acquired by SAP. As of version 7.500, released April 2007, the project has 940 thousand lines of C source code\footnote{generated using David A. Wheeler's `SLOCCount'.}. There are approximately ... developers working on it. MySQL 3.23, released in early 2001, contains 320 thousand lines of C and C++ source code. The MySQL project started in 1994.

\subsection{Generating the data}
From each project, we used source code commit data (the message attached to the commit). We leveraged the data we gathered in \cite{Hindle09ICSM} for this work. A typical commit message is: ``history annotate diffs bug fixed (if mysql\_real\_connect() failed there were two pointers to malloc'ed strings, with memory corruption on free(), of course) ". We extracted these messages and indexed them by creation time. We apply stop-word removal to eliminate common English words like `the', `at', and so on.

From that dataset, we created an XML file which grouped commits into temporal periods, each 30 days wide. This size of period is smaller than the time between minor releases but large enough for there to be sufficient commits to analyze. Given a text file full of commits, indexed by date and time, we then ran an LDA modeler on each period, to generate a list of topics per period. E.g., in Period 1, the most important topic might have been ``security", with ``sql'' second, down on to less relevant topics. We arbitrarily set the number of topics to generate to 20, because past experimentation showed that fewer topics might aggregate multiple unique topics while any more topics seemed to dilute the results and create indistinct topics.

\subsection{Associating labels}
% We would like to `name' our topic models for each period. In this way, we can see how certain words -- subjects -- occur more or less often with each period.
Once we had topics for each period, we tried to associate them with a label from a list of keywords and related terms. Recall that topics are lists of words and associated frequencies. We performed simple string matching between these topics and our lists, `naming' a topic if it contained that word or words. We used several different word lists for comparison. 

Our first list, \textsf{exp1}, was generated using the ontology described in Kayed et al.~\cite{5072519}. That paper constructs an ontology for software quality measurement using eighty source documents, including research papers and international standards. The labels we used:
\begin{quotation}
\small \noindent \textsf{
integrity, security,
interoperability, testability, maintainability, traceability,
accuracy, modifiability, understandability, availability, modularity,
usability, correctness, performance, verifiability, efficiency,
portability, flexibility, reliability.
}
\end{quotation}

Our second list relied on the ISO quality model~\cite{iso9126}. It describes six high-level quality requirements (listed in Table \ref{tbl:wnsig}). There is some debate about the significance and importance of the terms in this model. However, it is ``an international standard and thus provides an internationally accepted terminology for software quality \cite[p. 58]{Bøegh2008},'' which is sufficient for the purposes of this research. However, these terms may not capture all words associated with the labels.  For example, the term ‘user friendly’ is one most would agree is relevant to discussion of usability, but is not in the standard. Therefore we took the words from the taxonomy and expanded them.

%Cite mockus et al
To construct our expanded sets, we also used Wordnet~\cite{Fellbaum1998}, an English-language ‘lexical database’ that contains semantic relations between words, including meronymy and synonymy. We then added Boehm’s 1976 software quality model \cite{Boehm+:1976:ICSE}, and classified his eleven ‘ilities’ into their respective ISO9126 qualities. We did the same for the quality model produced by McCall et al. \cite{mccall1977}. Finally, we analyzed two mailing lists from the KDE project to enhance the specificity of the sets. We selected KDE-Usability, which focuses on usability discussions for KDE as a whole; and KDE-Konqueror, a list about a long-lived web browser project. For each high-level quality in ISO9126, we first searched for our existing labels; we then randomly sampled twenty-five mail messages that were relevant to that quality, and selected co-occurring terms relevant to that quality. For example, we add the term ‘performance’ to the synonyms for \emph{Efficiency}, since this term occurs in most KDE mail messages that discuss efficiency.
%exp2

For the third -- \textsf{exp3} -- list of quality labels, we extended the list from \textsf{exp2} using unfiltered Wordnet similarity matches. Similarity in Wordnet means siblings in a hypernym tree. We do not include these words here for space considerations (but see the Appendix for our data repository). It isn't clear the words associated with our labels are specific enough, however: for example, the label \emph{maintainability} is associated with words \emph{ease} and \emph{ownership}.

\begin{table*}[h]
	\caption{Qualities and associated signifiers – Wordnet version (\textsf{exp2})}
	\centering
	\label{tbl:wnsig}
\begin{tabular}{c|p{9cm}}
\toprule
\textbf{Label} & \textbf{Related terms} \\
\midrule
\emph{Maintainability} &
testability changeability analyzability stability maintain maintainable modularity modifiability understandability + interdependent dependency encapsulation decentralized modular\\ \hline
\emph{Functionality} &
security compliance accuracy interoperability suitability functional practicality functionality + compliant exploit certificate secured “buffer overflow” policy malicious trustworthy vulnerable vulnerability accurate secure vulnerability correctness accuracy\\ \hline
\emph{Portability} &
conformance adaptability replaceability installability portable movableness movability portability + specification migration standardized l10n localization i18n internationalization documentation interoperability transferability\\ \hline
\emph{Efficiency} &
“resource behaviour” “time behaviour” efficient efficiency + performance profiled optimize sluggish factor penalty slower faster slow fast optimization\\ \hline
\emph{Usability} &
operability understandability learnability useable usable serviceable usefulness utility useableness usableness serviceableness serviceability usability + gui accessibility menu configure convention standard feature focus ui mouse icons ugly dialog guidelines click default human convention friendly user screen interface flexibility\\ \hline
\emph{Reliability} &
“fault tolerance” recoverability maturity reliable dependable responsibleness responsibility reliableness reliability dependableness dependability + resilience integrity stability stable crash bug fails redundancy error failure\\ 
\bottomrule
\end{tabular}
\end{table*}

\subsection{Supervised learning}
To check our outcomes, we manually annotated each topic in each period with the same quality labels as \textsf{exp2}. We looked at each period's topics, and assessed what the data -- consisting of the frequency-weighted word lists and messages -- suggested was the topic for that period. For example, for the MaxDB topic consisting of a message ``exit() only used in non NPTL LINUX Versions'', we tagged that topic \emph{portability}. This provided us with manually classified topics for the second phase of our analysis, supervised learning (classification).
%inter-rater reliability 

We first compared our previous analysis using label matching to our manual classifications to get an error rate for that process (described below). 

For supervised learning, we used a suite of supervised classifiers from WEKA~\cite{weka09}. WEKA contains a suite of machine learning tools (e.g., support vector machines and Bayesian nets). We also used the multi-labeling add-on for WEKA, Mulan\footnote{\url{http://mlkd.csd.auth.gr/multilabel.html}}. Traditional classifiers map our topics to a single class, whereas Mulan allows for a mixture of classes per topic (much like our manual classification found overlaps). 

To assess the performance of the supervised learners, we did a 10-fold cross-over analysis (using 1 period to train the learner on the tags, and the other 9 to attempt to learn, changing the training set each time). We report these results below.

% enumerate?
Finally, using this data, we evaluated two research questions: 1) Do label frequencies change over time? That is, is a certain quality of more interest at one point in the lifecycle than some other? 2) Do the different projects differ in their relative topic interest? That is, is a particular quality more important to one project than the other projects?  

\section{Observations and evaluation}

\subsection{Word list similarity}

% word list similarity

In general, this approach did not work out well. The related words for \emph{correctness}, for example, tended to be too lengthy and non-specific. Table \ref{tbl:wordlist} lists results. A \emph{named topic} is a topic with a matching label. There are \{periods X 20\} topics per project. All experiments were run on MaxDB data.

\begin{table*}[h]
	\caption{Automatic topic labeling}
	\centering
	\label{tbl:wordlist}
\begin{tabular}{c|c|c|c}
\toprule
Measure &		 \textsf{exp1} &	 \textsf{exp2} &	 \textsf{exp3} \\
\midrule
Topics &			500		 &			500  	 & 		500  \\
Named topics &		281      &			125      &		328  \\
Unnamed topics &	139      &			295      &      92   \\
\bottomrule
\end{tabular}
\end{table*}

For \textsf{exp1}, our best performing labels (the labels matched with the most topics) were correctness (182 topics) and testability (121). We did not get good results for usability or accuracy, which were associated with fewer than ten topics. We also looked for correlation between our labels: Excluding double matches (self-correlation), our highest co-occurring terms were verifiability and traceability, and testability and correctness (76 and 62 matches, respectively).

For \textsf{exp2}, there are many more unnamed topics. Only reliability produces a lot of matches, mostly with the `error' word. Co-occurrence results were poor.

For \textsf{exp3}, we had many more named topics. As we mentioned, the wordlists are quite broad, so there are likely to be false-positives. See the following sections for our error analysis. We found a high of 265 topics for usability, with a low of 44 topics for maintainability. Common co-occurrences were reliability and usability, efficiency and reliability, and efficiency and usability (200, 190, and 150 topics in common, respectively). 

\subsubsection{Analysing the unsupervised labeling}
Based on the labels, and our manual topic labeling, we compared the results of the unsupervised word matching approach. For each quality we tried to assess whether the manual tag matched the unsupervised label assigned. Table \ref{tbl:maxdb-unsup-results} shows our results for MaxDB. In general results are poor. Using the F-Measure, the weighted average of precision and recall, where 1 is perfect, our best results are 0.6, a few at 0.4, and most around 0.2. Similar results using the Matthew's correlation coefficient (used to measure efficacy where classes are of different sizes), and the receiver operating characteristic area under curve. 

\begin{table*}[h]
	\caption{Results for automatic topic labeling. f1 = f-measure, mcc = Matthew's correlation coeff., roca = ROC area}
	\centering
	\label{tbl:maxdb-unsup-results}
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
Experiment & label & f1 & mcc & precision & recall & roca  \\ 
\midrule
MaxDB exp2 & portability & 0.228 & 0.182 & 0.520 & 0.146 & 0.553 \\ 
 & efficiency & 0.217 & 0.125 & 0.237 & 0.200 & 0.558\\ 
 & reliability & 0.380 & 0.340 & 0.246 & 0.829 & 0.765 \\ 
 & functionality & 0.095 & 0.083 & 0.250 & 0.059 & 0.521 \\ 
 & maintainability & 0.092 & 0.123 & 0.571 & 0.050 & 0.520 \\ 
 & usability & 0.175 & 0.138 & 0.113 & 0.389 & 0.620 \\ 
 & total & 0.236 & 0.127 & 0.248 & 0.225 & 0.561 \\ 
\midrule
mysql exp2 & portability & 0.138 & 0.211 & 1.000 & 0.074 & 0.537 \\ 
 & efficiency & 0.345 & 0.327 & 0.476 & 0.270 & 0.625 \\ 
 & reliability & 0.425 & 0.287 & 0.348 & 0.545 & 0.669 \\ 
 & functionality & 0.025 & 0.006 & 0.571 & 0.013 & 0.501 \\ 
 & maintainability & 0.000 & 0.000 & 0.000 & 0.000 & 0.500 \\ 
 & usability & 0.175 & 0.135 & 0.200 & 0.156 & 0.560 \\ 
 & total & 0.167 & 0.095 & 0.403 & 0.105 & 0.527 \\ 
\midrule
maxdb exp3 & portability & 0.472 & 0.286 & 0.402 & 0.573 & 0.660 \\ 
 & efficiency & 0.223 & 0.068 & 0.130 & 0.778 & 0.549 \\ 
 & reliability & 0.257 & 0.196 & 0.149 & 0.927 & 0.652 \\ 
 & functionality & 0.236 & 0.187 & 0.138 & 0.824 & 0.665 \\ 
 & maintainability & 0.338 & 0.112 & 0.266 & 0.463 & 0.566 \\ 
 & usability & 0.108 & 0.094 & 0.057 & 0.944 & 0.595 \\ 
 & total & 0.258 & 0.093 & 0.160 & 0.671 & 0.568 \\ 
\midrule
mysql exp3 & portability & 0.413 & 0.170 & 0.564 & 0.325 & 0.574 \\ 
 & efficiency & 0.158 & 0.105 & 0.089 & 0.703 & 0.608 \\ 
 & reliability & 0.388 & 0.240 & 0.260 & 0.758 & 0.660 \\ 
 & functionality & 0.652 & 0.240 & 0.655 & 0.649 & 0.620 \\ 
 & maintainability & 0.203 & 0.007 & 0.240 & 0.175 & 0.503 \\ 
 & usability & 0.105 & 0.013 & 0.057 & 0.688 & 0.513 \\ 
 & total & 0.362 & 0.076 & 0.284 & 0.499 & 0.544 \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Classifying topics}
We present a ROC chart in Fig. \ref{}. The chart shows each learner measured in terms of precision and recall. In general, we prefer techniques which are closer to the upper-left corner (maximizing precision and maximizing recall). 

\subsection{Comparing topic use between projects}

\subsection{Topic change over time}

\subsection{Threats to validity}
\textbf{Construct validity} – Focus on code commit messages, rather than mail or bug trackers. Choice of taxonomy

\textbf{Internal validity} -- inter-rater reliability.

\textbf{External validity} – Our data originated from open-source projects in the database domain. Of these, the open-source nature of the project seems most problematic for external validity. However, both projects are developed by commercial entities. Our technique is dependent on development practices including meaningful commit messages, which might not be true in other domains.

\section{Related work}
Part of our effort with this project is to understand the qualitative and intentional aspects of requirements in software evolution, a notion we first discussed in \cite{ernst07icsm}. That idea is derived from, in part, work on narratives of software systems shown in academic work like \cite{anton01}, or more general-purpose works like \cite{waldo93}.

The idea of extracting higher-level `concerns' (also known as `concepts', `aspects' or `requirements') has been attacked in two ways.

Cleland-Huang and her colleagues published work on mining requirements documents for non-functional requirements (quality requirements) in \cite{Cleland-Huang2006}. One approach they tried was similar to this one, with keywords mined from NFR catalogues found in~\cite{chung99}. They managed recall of 80\% with precision of 57\% for the Security NFR, but could not find a reliable source of keywords for other NFRs. Instead, they developed a supervised classifier by using human experts to identify an NFR training set. There are several reasons we did not follow this route. One, we believe we have a more comprehensive set of terms due to the taxonomy we chose. Secondly, we wanted to compare across projects. Their technique was not compared across different projects and the applicability of the training set to different corpora is unclear. A common taxonomy allows us to make inter-project comparison (subject to the assumption that all projects conceive of these terms in the same way). Thirdly, while the objective of Cleland-Huang's study was to identify new NFRs (for system development) our study assumes these NFRs are latent in the textual documents of the project. Finally, the source text we use is less structured than their requirements documents.

Mockus and Votta studied a large-scale industrial change-tracking system in ~\cite{Mockus00}. They also leveraged Wordnet, but only for word roots. They felt the synonyms would be non-specific and cause errors. A nice contribution was access to system developers, with whom they could validate their labels. Since we try to bridge different organizations, these interviews are infeasible (particularly in the distributed world of open-source software).

The other approach is to start with code repositories, and try to extract concerns from there. In \cite{marcus04wcre}, the authors describe their use of Latent Semantic Indexing to identify commonly occurring concerns for software maintenance. Some results were interesting, but their precision was quite low. ConcernLines~\cite{treude09cl} shows tag occurrence using color intensity. They mined change request tags (such as `milestone 3') and used these to make evolutionary analyses of a single product. The presence of a well-maintained set of tags is obviously essential to the success of this technique.
%We maintain that these results require more substantive manipulation before being reported to, for example, management. Like any large-scale data analysis project, it requires skill to derive the implications. Hooking the backend results directly to a reporting tool is unlikely to produce useful results.

% There has been an explosion of interest in mining OSS repositories. Godfrey and Tu \cite{godfrey00} were among the first to use this data to assess software evolution. They discussed how well the growth of Linux was predicted by laws that Lehman and colleagues \cite{lehman85} proposed. More recently, Herraiz et al. \cite{herraiz07icsm} predicted OSS growth using time series analysis. Like us, they refer to release windows as a useful unit of analysis. 

Mens et al. \cite{mens08icsm} conducted an empirical study of Eclipse, the OSS code editor, to verify the claims of Lehman~\cite{lehman97sms}. They concerned themselves with source code only, and found Law Seven, ``Declining Quality'', to be too difficult to assess: ``[we lacked an] appropriate measurement of the evolving quality of the system as perceived by the users \cite[p. 388]{mens08icsm}''. This paper examines the notions of quality in terms of a consistent ontology, as Mens et al. call for in their conclusions.

In \cite{Mei2007}, the authors use context information to automatically name topics. They describe probabilistic labeling, using the frequency distribution of words in a topic to create a meaningful phrase. They do not use external domain-specific information as we do.

Massey~\cite{massey02icse} and Scacchi (\cite{scacchi02,scacchi05b}) looked at the topic of requirements in open-source software. Their work discusses the source of the requirements and how they are used in the development process. German~\cite{german03gnome} looked at GNOME specifically, and listed several sources for requirements: leader interest, mimicry, brainstorming, and prototypes. None of this work  addressed quality requirements in OSS, nor did it examine requirements trends.

Hindle et al. \cite{Hindle2007} examined release patterns in OSS. They showed that there is a difference between projects regarding maintenance techniques. This supports our result that software qualities are not discussed with the same frequency across projects.

Most of this related research examined project source code. Software mining of other project corpora is less common. Similarities exist with approaches that begin with structured taxonomies, as with the Hismo software ontology \cite{girba06}. Rigby and Hassan \cite{rigby07msr} used a general purpose taxonomy to classify developer email according to temperament.

\section{Conclusions and future work}
In this paper we showed that a domain specific wordnet for software development provides better accuracy for labelling topic models than unsupervised approaches.

This paper has not considered the interpretation of attitudes towards qualities (e.g., do developers feel this helps security).

It is difficult to map abstract qualities to particular messages. If the message is about a particular bug fix, do we map that to reliability? Maintainability? Is it a performance bug or a usability bug? Is a `quality' discussion about more than just corrective maintenance, in other words? In this study we take the position that any type of maintenance has to do with software quality.

\section{Acknowledgements and appendix}
Our data and scripts are available at \url{http://softwareprocess.es/icpc2010}

\bibliographystyle{IEEEtran}
\bibliography{icpc}

\end{document}


\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{times}


\usepackage{graphicx,amsmath,comment,fullpage}
\newcommand{\igH}[1]{\includegraphics[height=.9\textheight]{#1}}
\newcommand{\igW}[1]{\includegraphics[width=.9\textwidth]{#1}}
\newcommand{\igWh}[1]{\includegraphics[width=.7\textwidth]{#1}}
\newcommand{\igWhalf}[1]{\includegraphics[width=.45\textwidth]{#1}}
\newcommand{\lda}{Latent Dirichlet Allocation}


\author{Abram Hindle and Michael Godfrey and Ric Holt \\
\{ahindle,migod,holt\}@cs.uwaterloo.ca\\
Software Architecture Group (SWAG)\\
University of Waterloo\\
}
\title{Windowed Developer Topic Analysis}

\usepackage{rotating}   
\begin{document}

\maketitle
\begin{abstract}

  At different times, developers focus on different topics and tasks
  during development.  These different topics and times of focus might
  be obvious to developers familar with the project, but other
  stakeholders, like managers, might be less aware of the topics of
  development. Imagine the scenario where developers had to deal with
  issues orthogonal to the features suggested, at the end of the
  iteration a manager would wonder where all the time went, the
  developers might have just blocked all sides issues in with features
  they were initially working on, they might not have noticed their
  focus had changed.  Currently it is difficult for stakeholders to
  audit what actually occurred and when certain features or topics
  were focussed on. We propose and provide a visualization of the
  changing development topics over time. Our approach takes a topic
  analysis tool like Latent Dirichlet Allocation (LDA) or Latent
  Semantic Indexing (LSI) and instead of applying the tool to the
  entire corpus we apply to windows of the corpus. This allows us to
  see the unique topics of development occuring during a particular
  time period and then see if these individual topics ever reappear.
  





%   In this lab report we seek to explore how topics shift over time in
%   the source control repository of a project. We analyze commit
%   comments, and even source code per revision per each time window and
%   extract the topics prevelant in that window. We expect that topic
%   sets will change over time and the change in topic sets indicate a
%   change in the focus of development. We suppose this could help us to
%   segment the revisions on the time line to discover parts of an
%   iteration. For topic extraction we use \lda (LDA).

\end{abstract}

\section{Introduction}

%?


% 

% In the presence of fine grained changes and detailed changelogs

% Developers topics the focus of develop

% many changelogs how to sort by change type

% interleaved changes relate similar changes

What happened in the previous iteration? What were the developers
working on? What was the developer in the next cubicle working on?
What were the common topics and issues dealt with in the current
release of the software? What were the topics of the previous release?
Given the requirements agreed to at the start of the iteration did our
developers work on them? What else did they work on. Were there issues
which were orthogonal to the

Topic analysis, with respect to Software Control Systems (SCSs), is
valuable as it has many uses for the stakeholders involved. 

Example scenarios where the usage of this technique would prove useful
are numerous. Imagine a development team has agreed to implement some
features at the start of an iteration. By the end of the iteration
only some of the agreed upon features are implemented. The developers
swear they were focusing solely on the features agreed upon. Topic
analysis could help by teasing out the distinct topics or focusses of
the developers. Perhaps the developers had to deal with issues
orthogonal to the features they were working on, but were probably
necessary to complete such features. Developers might have grouped
those orthogonal changes into one of the features and might not


We propose to extend common concept analysis techniques by executing
them over windows of documents over time. We hope by looking at topic
clusters over time we can I identify continuous effort on a select
group of topics.

Topic analysis of fine-grained revisions is important as often
revisions belonging to different topics are interleaved or even mixed
inside of commits.  Sometimes a single revision to a file contains
multiple bug fixes.  Using powerful topic analysis like Latent
Semantic Indexing (LSI) or Latent Dirichlet Allocation (LDA) one can
seperate these mixed signals into coherent topics. In this case a
topic is a set of tokens that are related to each other.

% We suppose this could help us to segment the revisions on the time
% line to discover parts of an iteration.

Topic analysis is useful because it can help us partition a project's
time-line into sub-iterations. By breaking apart an iteration we might
be able to extract the underlying software development process from
these artifacts.

In this paper we seek to explore how topics shift over time in
the source control repository of a project. We analyze commit
comments, and even source code per revision per each time window and
extract the topics in that time window. We expect that topic sets
will change over time and the change in topic sets indicate a change
in the focus of development. 



\section{Background}


Some terms we will be using for the rest of the paper include:
message, word distribution, topic and trend. Messages are block of
text written by developers, in this paper messages will be the CVS
commit comments. Word distributions are the summary of a message by
word count, effectively the word distribution will be over all
messages looked at but since most words will not appear in a message,
a word distribution is effectively a word count divided by the message
size. A topic is a unique word distribution, but effectively a set of
words unique to an independant behaviour in the text corpus, in this
paper we often summarize topics by their top 10 more frequent words.
A trend is a one or more similar topics which reoccur over
time. Trends are important because they are sets of topics which
reoccur over the development of a project.


%\subsection{Intro to LDA}

%XXX explain LDA \cite{944937}


Latent Dirichlet Alloation (LDA)~\cite{944937} is a competitor to
latent semantic indexing (LSI), probabilistic latent semantic indexing
(pLSI) and semantic clustering. It is used for document modelling,
document clustering and collaborative filtering. What LDA attempts to
do is model a documents as mixture model of topics.  The common use of
LDA in software engineering
literature~\cite{lukins2008,10.1109/MSR.2007.20,NIPS2007637,1321709}
is to extract topic clusters from documents such as methods, bug
reports, source code and files.

Our blackbox view of LDA and LSI is that they extract topic clusters
from the documents that are independant from one another. LDA doesn't
name these clusters but the clusters consist of words whose
relationship is often obvious to someone familar with the
corpus. Technically we could swap LDA for LSI. Our data is posed as
documents with word distributions (word counts per documents) and LDA
extracts distinct topics (clusters of words) from the documents.

\subsection{LDA, LSI and Semantic Clustering}

Linstead et al has proposed an author source code model using
LDA~\cite{10.1109/MSR.2007.20,NIPS2007637,1321709}. They used an
author-topic (AT) model~\cite{1036902} to infer the association or
expertise of a particular developer with a particular code topic
extracted from the source control system.

Lukins, Kraft and Etzkorn~\cite{lukins2008} use LDA to help query for
finding related documents during bug localization tasks. They used LDA
to build a topic model and then query against it using sample
documents, the query would the proportion of topics that were
associated with the query and thus the related documents.

LSI is related to LDA and has been used to identify topics in software
artifacts for forumal concept analysis and concept
location~\cite{1421013,1374321,10.1109/ICPC.2007.13,10.1109/ICPC.2006.17}.
This is when concepts are automatically extracted from source code or
documents are related to part of the source code.  Concept location is
how high level concepts relate to low level entities like source code,
for instance when fixing a bug how do the concepts in the bug report
map back to the source code?  Semantic Clustering has also been used
for similar reason~\cite{1698774,1566153}.

Grant et al.~\cite{scottcordy} have used an alternative technique to
LSI and LDA, they used Independant Component Analysis to seperate
topic signals from software code.

Our technique differs in the sense we apply LDA temporally and we
apply to change comments.

%      1. [ ] LDA itself
%      2. [ ] Maletic stuff
%      3. [ ] WCRE: Scott/Jim
%      4. [ ] WCRE: Bug LDA stuff
%      5. [ ] MSR: Challenge work
%      6. [ ] FCA work
%4. [ ] Methodology [0/9]



\section{Methodology}

\subsection{Extract Repos}

We extracted the repositories with software such as rsync, CVSSuck,
\texttt{softChange} and bt2csv.  \texttt{softChange} provided a
general schema for storing revisions. CVSSuck and rsync mirrored CVS
repositories while bt2csv mirrored web accessible bitkeeper
repositories.


%   2. [ ] Extract Documents
\subsection{Extract Documents}
%   3. [ ] Windowed Sets

The document we are primarily using are the changelog comments, these
are the comments added when a revision to the source code is
committed, it could be per file or per commit of changes to multiple
files, this depends on the source control system used.

Documents are converted into word count distributions. The
distributions include all of the possible words in the system.

\subsection{Windowed Sets}

We then create windows which are sets of documents grouped by time
window. Time windows can overlap but don't have to. Windowing by time
allows for many levels of granularity. We usually used one month as
the length of our time windows.

%   4. [ ] Cluster Windowed Set w/ LDA
\subsection{Cluster Windowed Set w/ LDA}
%   5. [ ] Extract Clusters

We take our topic clusterer and cluster the documents by their topics
and get the $n$ unique topic clusters out. In this paper we used LDA
but LSI could be used in a similar way.

This is a very slow step as LDA takes a long time to run.

\subsection{Extract Clusters}


%   6. [ ] Cluster Similarity [0/1]
\subsection{Cluster Similarity}
%      1. [ ] need the tool
%             grab from the cluster analysis
%   7. [ ] Continuous Cluster Analysis


Once we get our clusters, we analyze them and look for repeating
topics. We do this by taking the top 10 most common words and
comparing them. Given a fraction such as 8/10 we joined topics where
8/10 of the top ten topic words were shared.

Clusters in the same period which are this similar are grouped
together. Anything that matches either of those clusters 8/10 will
match.



\subsection{Continuous Cluster Analysis}

We extend the cluster matching across contiguous months.q

%   8. [ ] Topic Smear
\subsection{Topic Smear}
%   9. [ ] Visualization [0/1]
\subsection{Visualization}
%       1. [ ] need the visualizer



\subsection{Datasets and Tools}

To extract data from CVS and BitKeeper we used softChange to extract
CVS repositories and provide a schema for our revision data. We used
bt2csv to extract BitKeeper revisions from BitKeeper web repositories.

% Extractors:
% \begin{itemize}
% \item CVSSuck - CVS Suck mirrors RCS files from a CVS repository. 
% \item softChange - extract CVS facts to a PostgreSQL database.
% \item bt2csv - Convert BitKeeper repositories to facts in CSV
%   databases.
% \end{itemize}

Our analysis tools consisted of: Hiraldo-Grok, an OCaml based spin off
of Grok used for answering queries; Gnuplot, a graph plotting package,
lda-c a LDA package by Biel


%   1. [ ] LDA

%   2. [ ] Extractor

%   3.   Similarity

%   4.   Smearing

%   5.   Visualizer



\section{First Impressions}

In our first exploratory pass we wanted to see 

Looking at 30 day non-overlapping windows of revisions for MySQL 3.23
(20 topics, top 10 words per topic) we found there were common words
across topic clusters such as diffs, annotate and history. There were
notable transitional topics such as in the first window the word
``bitkeeper'' appears (probably when they adopted Bitkeeper for their
source control system) yet in the following windows there were no
mentions of Bitkeeper. ``RENAME'' also only appeared once in the first
window and never again.

A subsampling of the notable topic words is displayed in Table
\ref{tab:portability}. We tracked one topic cluster which seemed
related to portability.

%Often words are shared across topics, see table \ref{tab:portability}
%for tracking topics about portability.

After viewing these topic is was obvious that we would have to combine
try to track the evolution of topics over the windows or rank topics
by similarity.

%It is apparent that we have to compare these topics some how by
%itemset or similarity.

\begin{table*}
\centering
\begin{tabular}{|ccc|l|}
\hline
2000 &  Jul &  31 &    chmod \\
2000 &  Sep &  29 &    fixes benchmark logging windows \\
2000 &  Nov &  28 &    typo fix insert\_multi\_value \\
2001 &  Jan &  27 &    fixes Innobase Cleanups auto-union \\
2001 &  Mar &  28 &    2 topics bugfix, logging , TEMPORARY,  \\
\hline
2001 &  Jul &  26 &    update Allow TABLES LOCK [a] \\ 

2001 &  Aug &  25 &    tables row version [a] \\
\hline
2001 &  Sep &  24 &    update checksum merge \\
2001 &  Oct &  24 &    fixed fix \\
2001 &  Dec &  23 &    HPUX SCO fix \\
\hline
2002 &  Feb &  21 &    net buffer length  max\_allowed\_packet [b] \\
2002 &  Mar &  23 &    small buf fix [b]  \\
\hline
2002 &  May &  22 &    [popular] fix SCO OSF1 table\_name \\
2002 &  Nov &  18 &    HPUX11 compiler HP \\
\hline
2003 &  Feb &  16 &    Linux errno  [c] \\
2003 &  Mar &  18 &    alarm bookmark bug [c] \\
\hline
2003 &  Sep &  14 &    Auto logging merge windows distribution fix 64-bit 4.0 Cleanup \\
\hline
\end{tabular}
\caption{Tracking topics associated with the word portability, note some continuous blocks}
\label{tab:portability}
\end{table*}

%LDA Skeleton
%1. [ ] Abstract 
%2. [ ] Introduction

%3. [ ] Previous Work [0/2]
%   2. [ ] LDA Work [0/6]
\section{Results}
%   1. Interesting smears 
%   2.   postgresql
\subsection{Postgresql}
%   3.   mysql
\subsection{Mysql}
%   4.   firebird
\subsection{Firebird}
%   5.   what's that other db?
\subsection{ what's that other db?}
%7.   Validation [0/1]
\subsection{Validation}
%   1. well i'm stumped
%   2. look at it?
%   3. investigate those revisions and check the proportion etc
%   4. exploratory
%   5.   work it out
%8.   Validity Threats [0/5]
\section{Validity Threats}
%   1.   validation
%   2.   LDA is questionable
%   3.   blackbox problem
%   4.   not in the token
%   5.   multiple X token
%9.    Future Work
\section{ Future Work}
%10.   Conclusions
\section{Conclusions}
%11.   Start file file:/home/abez/projects/lda-paper/lda-paper.tex


\bibliographystyle{alpha}
\bibliography{lda-paper}


\end{document}
